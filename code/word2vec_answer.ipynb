{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_answer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAICRJCxhe0v",
        "colab_type": "text"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2wQjBIxgNOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive') # in case you want to save reaults to your drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnghwfpchHOw",
        "colab_type": "code",
        "outputId": "8cff9d1c-f4f3-469e-9404-0633b5e4bc63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8iKBfsTbsuu",
        "colab_type": "code",
        "outputId": "ac4607ac-c3b6-431b-d884-38cff24119d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "\n",
        "!wget http://mattmahoney.net/dc/text8.zip\n",
        "!unzip text8.zip\n",
        "\n",
        "!wget https://raw.githubusercontent.com/uhh-lt/dl-seminar/master/code/wikipedia-corpus-2mb.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-24 07:46:02--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M  2.17MB/s    in 14s     \n",
            "\n",
            "2019-05-24 07:46:16 (2.15 MB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n",
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_v7Rydw52qe",
        "colab_type": "text"
      },
      "source": [
        "# Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN2H2WSRWoj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "corpus_path = './text8'\n",
        "\n",
        "with open(corpus_path) as f:\n",
        "  corpus_raw = [w.lower() for w in nltk.word_tokenize(f.read())]\n",
        "\n",
        "n = 200\n",
        "corpus_token = []\n",
        "for i in range(0, len(corpus_raw), n):\n",
        "    corpus_token.append(corpus_raw[i:i + n])\n",
        "    \n",
        "\n",
        "# It is also possible to use the following code:\n",
        "\n",
        "# from gensim.models.word2vec import Text8Corpus\n",
        "# corpus_token = Text8Corpus(corpus_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh7_aUobXTV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "model = Word2Vec(corpus_token, size=100, window=2, min_count=3, workers=4, iter=3)\n",
        "model.save('gensim_word2vec.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzOUCl5EX4bH",
        "colab_type": "code",
        "outputId": "d7d37386-fa97-4978-ccc2-4ff5eb311368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "model = Word2Vec.load('gensim_word2vec.model')\n",
        "\n",
        "# TODO: find similar words to word 'three'. You can use gensim 'most_similar' function\n",
        "model.wv.most_similar(positive=['one'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('seven', 0.9008585214614868),\n",
              " ('six', 0.893919825553894),\n",
              " ('four', 0.8925461769104004),\n",
              " ('eight', 0.8894898295402527),\n",
              " ('five', 0.883519172668457),\n",
              " ('three', 0.8646017909049988),\n",
              " ('nine', 0.8602133989334106),\n",
              " ('two', 0.8056491613388062),\n",
              " ('zero', 0.7678817510604858),\n",
              " ('july', 0.7173868417739868)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3B95Kv2hjxV",
        "colab_type": "text"
      },
      "source": [
        "# Utility code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iLYvHXlgkt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "\n",
        "\n",
        "def ensure_dir(f):\n",
        "    if not os.path.exists(f): \n",
        "      os.makedirs(f)\n",
        "      \n",
        "\n",
        "def load_corpus(filename, lower_case=True, min_frequency=3):\n",
        "    \"\"\" Load a text file, tokenize it, count occurences and build a word encoder \n",
        "    that translate a word into a unique id (sorted by word frequency) \"\"\"\n",
        "    \n",
        "    corpus = []\n",
        "    \n",
        "    i = 0\n",
        "    with open(filename, 'r') as in_file:\n",
        "        for line in in_file:\n",
        "            if i % 1000 == 0:\n",
        "                print('Loading {} processing line {}'.format(filename, i))\n",
        "            \n",
        "            if line[-1] == '\\n':\n",
        "                line = line[:-1]\n",
        "            line = line.strip()\n",
        "            if lower_case:\n",
        "                line = line.lower()\n",
        "            \n",
        "            corpus += nltk.word_tokenize(line)\n",
        "            i += 1\n",
        "    \n",
        "    print('Compute word encoder...')\n",
        "    word_counter = defaultdict(int)\n",
        "    \n",
        "    for word in corpus:\n",
        "        word_counter[word] += 1\n",
        "    \n",
        "    word_counter = list(word_counter.items())\n",
        "    word_counter = [elem for elem in word_counter if elem[1] >= min_frequency]\n",
        "    word_counter.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    word2index = defaultdict(int)\n",
        "    \n",
        "    for i, elem in enumerate(word_counter):\n",
        "        word2index[elem[0]] = i\n",
        "        \n",
        "    print('done')\n",
        "    \n",
        "    return corpus, word2index\n",
        "\n",
        "\n",
        "def save_vocabulary(output_dir, word2index):\n",
        "    vocab_fpath = os.path.join(output_dir, 'vocabulary.tsv')  \n",
        "    vocab_items = list(word2index.items())\n",
        "    vocab_items.sort(key=lambda x:x[1])\n",
        "    print(vocab_items[:100])\n",
        "    vocab_list = [elem[0] for elem in vocab_items if elem[1] > 0]\n",
        "    \n",
        "    with open(vocab_fpath, 'w') as vocab_file_out:\n",
        "        vocab_file_out.write('<UNK>'+'\\n')\n",
        "        for word in vocab_list:\n",
        "            vocab_file_out.write(word+'\\n')\n",
        "\n",
        "    print(\"Saved vocabulary to:\", vocab_fpath)\n",
        "    \n",
        "    return vocab_fpath"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yfw-Z_YhrRG",
        "colab_type": "text"
      },
      "source": [
        "# Model and training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-HGFrIxP7lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "from tensorflow.nn import sigmoid_cross_entropy_with_logits\n",
        "\n",
        "\n",
        "def build_graph2(vocabulary_size, num_sampled, embedding_size, \n",
        "                learning_rate, optimizer_type):\n",
        "    print('Using custom nce_loss function.')\n",
        "  \n",
        "    contexts = tf.placeholder(tf.int32, shape=[None])\n",
        "    targets = tf.placeholder(tf.int32, shape=[None, 1])\n",
        "    \n",
        "    embeddings = tf.Variable(\n",
        "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "    \n",
        "    nce_weights = tf.Variable(\n",
        "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
        "    \n",
        "    # TODO: implement noise contrastive estimation loss. Use tf.stop_gradients\n",
        "    # on sampled negative indices. We suggest using tf.nn.log_uniform_candidate_sampler to sample\n",
        "    # negative indices according to distribution of tokens in the corpus.\n",
        "    # Hint: you can always look into tf.nn.nce_loss code at github\n",
        "    \n",
        "    embed = tf.nn.embedding_lookup(embeddings, contexts)\n",
        "    \n",
        "    nce_embed_targets = tf.nn.embedding_lookup(nce_weights, \n",
        "                                               tf.reshape(targets, [-1]))\n",
        "    \n",
        "    negatives, _, _ = tf.nn.log_uniform_candidate_sampler(\n",
        "          true_classes=tf.cast(targets, tf.int64),\n",
        "          num_true=1,\n",
        "          num_sampled=num_sampled,\n",
        "          unique=True,\n",
        "          range_max=vocabulary_size-1,)\n",
        "\n",
        "    negatives = tf.stop_gradient(negatives)\n",
        "    nce_embed_negatives = tf.nn.embedding_lookup(nce_weights, \n",
        "                                                 tf.reshape(negatives, [-1]))\n",
        "    \n",
        "    neg_logits = tf.matmul(embed, nce_embed_negatives, transpose_b=True)\n",
        "    true_logits = tf.reduce_sum(tf.multiply(embed, nce_embed_targets), axis=1)\n",
        "    \n",
        "    loss = tf.reduce_mean(tf.log(1. + tf.exp(-true_logits)) + \n",
        "                          tf.reduce_sum(tf.log(1. + tf.exp(neg_logits)), axis=1))\n",
        "        \n",
        "    if optimizer_type == \"adam\":\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "    else:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "    \n",
        "    return embeddings, contexts, targets, optimizer, loss\n",
        "\n",
        "\n",
        "def build_graph(vocabulary_size, num_sampled, embedding_size, \n",
        "                learning_rate, optimizer_type):\n",
        "    print('Using built-in TF nce_loss function.')\n",
        "    \n",
        "    # Placeholders for inputs\n",
        "    contexts = tf.placeholder(tf.int32, shape=[None])\n",
        "    targets = tf.placeholder(tf.int32, shape=[None, 1])\n",
        "    \n",
        "    embeddings = tf.Variable(\n",
        "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "    \n",
        "    nce_weights = tf.Variable(\n",
        "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
        "    \n",
        "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "    \n",
        "    # TODO: generate embeddings of contexts \n",
        "    embed = tf.nn.embedding_lookup(embeddings, contexts)\n",
        "    \n",
        "    # TODO: compute the NCE loss, using a sample of the negative labels each time\n",
        "    # with tf.nn.nce_loss function (see TF documentation to find out what parameters you should use)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.nn.nce_loss(weights=nce_weights,\n",
        "                     biases=nce_biases,\n",
        "                     labels=targets,\n",
        "                     inputs=embed,\n",
        "                     num_sampled=num_sampled,\n",
        "                     num_classes=vocabulary_size))\n",
        "    \n",
        "    if optimizer_type == \"adam\":\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "    else:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "    \n",
        "    return embeddings, contexts, targets, optimizer, loss\n",
        "\n",
        "\n",
        "def generate_batch(corpus_num, batch_size, skip_gram=True):\n",
        "    \"\"\" Generate a batch in the form of two numpy vectors of (i) target \n",
        "    and (ii) context word ids. \"\"\"\n",
        "\n",
        "    contexts = np.ndarray(shape=(batch_size*2), dtype=np.int32)\n",
        "    targets = np.ndarray(shape=(batch_size*2, 1), dtype=np.int32)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        random_token_num = int(math.floor(np.random.random_sample() * (len(corpus_num) -2))) + 1\n",
        "        \n",
        "        # E.g. for \"the quick brown fox jumped over the lazy dog\"\n",
        "        # (context, target) pairs: ([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox)\n",
        "        # We can simplify to: (the, quick), (brown, quick), (quick, brown), (fox, brown), ... CBOW\n",
        "        # => contexts is ids of [the, brown, quick, fox, ...], labels/targets: [quick, quick, brown, brown, ...]\n",
        "\t# (quick, the), (quick, brown), (brown, quick), (brown, fox), ... Skip-gram\n",
        "        # => contexts and targets reversed\n",
        "        \n",
        "        # TODO: implement generation of left and right context pairs for CBOW \n",
        "        # according suggestions above\n",
        "        \n",
        "        # left context pair\n",
        "        left = [corpus_num[random_token_num - 1], corpus_num[random_token_num]]\n",
        "        \n",
        "        # right context pair\n",
        "        right = [corpus_num[random_token_num + 1], corpus_num[random_token_num]]\n",
        "        \n",
        "        if skip_gram:\n",
        "            # TODO: how we can transform left and right pairs to create SkipGram algorithm? \n",
        "            left.reverse()\n",
        "            right.reverse()\n",
        "        \n",
        "        contexts[i*2] = left[0]\n",
        "        contexts[i*2 + 1] = right[0]\n",
        "        \n",
        "        targets[i*2] = left[1]\n",
        "        targets[i*2 + 1] = right[1]\n",
        "    \n",
        "    return contexts, targets\n",
        "   \n",
        "  \n",
        "def train(corpus_num, word2index, vocabulary_size, num_samples, steps, \n",
        "          optimizer_type, learning_rate, embedding_size, skip_gram, \n",
        "          batch_size, save_path, use_custom_loss):   \n",
        "    with tf.device('/cpu'):\n",
        "        with tf.Session() as sess:\n",
        "            f_build_graph = build_graph2 if use_custom_loss else build_graph\n",
        "            \n",
        "            embeddings, contexts, targets, optimizer, loss = f_build_graph(vocabulary_size, \n",
        "                           num_samples, embedding_size, learning_rate, optimizer_type)\n",
        "            \n",
        "            # Save summary of the training process - can be analyzed with TensorBoard later \n",
        "            timestamp = str(int(time.time()))\n",
        "            logs_dir = os.path.join('w2v_logs_' + timestamp)\n",
        "            ensure_dir(logs_dir)\n",
        "            vocab_fpath = save_vocabulary(save_path, word2index)\n",
        "            \n",
        "            print('Writing summaries and checkpoints to logdir:' + logs_dir)\n",
        "            model_ckpt_fpath = os.path.join(logs_dir, 'model.ckpt')    \n",
        "            loss_summary = tf.summary.scalar('loss', loss) \n",
        "            config = projector.ProjectorConfig()\n",
        "            embedding = config.embeddings.add()\n",
        "            embedding.tensor_name = embeddings.name\n",
        "            embedding.metadata_path = vocab_fpath  \n",
        "            train_summary_op = tf.summary.merge_all()\n",
        "            summary_writer = tf.summary.FileWriter(logs_dir, sess.graph)\n",
        "            projector.visualize_embeddings(summary_writer, config)\n",
        "\n",
        "            # Initialization\n",
        "            saver = tf.train.Saver(tf.global_variables())\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            losses = []\n",
        "            \n",
        "            # Batched SGD training\n",
        "            for current_step in range(steps):\n",
        "                inputs, labels = generate_batch(corpus_num, batch_size=batch_size, skip_gram=skip_gram)\n",
        "                feed_dict = {contexts: inputs, targets: labels}\n",
        "                _, cur_loss = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
        "                \n",
        "                losses.append(cur_loss)\n",
        "                             \n",
        "                if current_step % 100==0 and current_step != 0:\n",
        "                    summary_str = sess.run(train_summary_op, feed_dict=feed_dict)\n",
        "                    summary_writer.add_summary(summary_str, current_step)\n",
        "                    \n",
        "                if current_step % 1000 == 0:\n",
        "                    print('step',current_step,'mean loss:', np.mean(np.asarray(losses)))\n",
        "                    saver.save(sess, model_ckpt_fpath, current_step)\n",
        "                    losses = []\n",
        "                    \n",
        "            embeddings_np = sess.run(embeddings)\n",
        "            np.save(os.path.join(save_path, 'embeddings.npy'), embeddings_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1yvMfb6iFmg",
        "colab_type": "text"
      },
      "source": [
        "# Launch training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_Rb8T8ag_QY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "OPTIONS = pd.Series()\n",
        "OPTIONS.corpus = \"wikipedia-corpus-2mb.txt\" # \"Path to the input text corpus. Change to 'text8' to train good embeddings.\"\n",
        "OPTIONS.num_neg_samples = 2    # \"Number of negative samples\"\n",
        "OPTIONS.steps = 100000         # \"Number of training steps\"\n",
        "OPTIONS.learning_rate = 1.     # \"The learning rate\"\n",
        "OPTIONS.embedding_size = 100   # \"Size of the embedding\"\n",
        "OPTIONS.lower_case = True      # \"Whether the corpus should be lowercased\"\n",
        "OPTIONS.skip_gram = False      # \"Whether skip gram should be used or CBOW\"\n",
        "OPTIONS.min_frequency = 3      # \"Words that occur lower than this frequency are discarded as OOV\"\n",
        "OPTIONS.optimizer_type = \"sgd\" # \"Optimizer type: 'adam' or 'sgd'\"\n",
        "OPTIONS.batch_size = 128       # \"Batch size\"\n",
        "OPTIONS.save_path = './'       # Path to directory to save results (dictionary, embedding matrice)\n",
        "OPTIONS.use_custom_loss = False # Switch to True if you want to do an advanced exercise\n",
        "                                # and implement nce loss by yourself in build_graph2 function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaLod8QSg5-L",
        "colab_type": "code",
        "outputId": "deb8eb82-abe5-4142-9417-a832f2a42c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "corpus, word2index = load_corpus(filename=OPTIONS.corpus, \n",
        "                                 lower_case=OPTIONS.lower_case, \n",
        "                                 min_frequency=OPTIONS.min_frequency)\n",
        "corpus_num = [word2index[word] for word in corpus]\n",
        "print(len(corpus_num))\n",
        "\n",
        "print('First few tokens of corpus:', corpus[:100])\n",
        "print('First few tokens of corpus_num:', list(corpus_num[:100]))\n",
        "\n",
        "corpus_num = np.asarray(corpus_num)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading text8 processing line 0\n",
            "Compute word encoder...\n",
            "done\n",
            "17007698\n",
            "First few tokens of corpus: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing']\n",
            "First few tokens of corpus_num: [5232, 3079, 11, 5, 194, 1, 3132, 45, 58, 155, 127, 741, 476, 10570, 133, 0, 27347, 1, 0, 102, 853, 2, 0, 15067, 58108, 1, 0, 150, 853, 3579, 0, 194, 10, 190, 58, 4, 5, 10711, 214, 6, 1323, 104, 454, 19, 58, 2730, 362, 6, 3671, 0, 708, 1, 371, 26, 40, 37, 53, 539, 97, 11, 5, 1422, 2756, 18, 567, 686, 7087, 0, 247, 5232, 10, 1051, 27, 0, 320, 248, 44607, 2876, 791, 186, 5232, 11, 5, 200, 602, 10, 0, 1133, 19, 2620, 25, 8982, 2, 279, 31, 4146, 141, 59, 25, 6437]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyjsdDLB6d5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1886
        },
        "outputId": "ace67c46-9f3f-4432-d510-ac8322fd48be"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "train(corpus_num, \n",
        "      word2index, \n",
        "      vocabulary_size=max(corpus_num) + 1,\n",
        "      num_samples=OPTIONS.num_neg_samples, \n",
        "      steps=OPTIONS.steps,\n",
        "      optimizer_type=OPTIONS.optimizer_type, \n",
        "      learning_rate=OPTIONS.learning_rate, \n",
        "      embedding_size=OPTIONS.embedding_size,\n",
        "      skip_gram=OPTIONS.skip_gram, \n",
        "      batch_size=OPTIONS.batch_size,\n",
        "      save_path=OPTIONS.save_path,\n",
        "      use_custom_loss=OPTIONS.use_custom_loss)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using built-in TF nce_loss function.\n",
            "[('the', 0), ('lahontan', 0), ('septentrionale', 0), ('anarchiste', 0), ('mutuellisme', 0), ('amoralism', 0), ('individualistically', 0), ('experimenal', 0), ('signficiant', 0), ('libertaire', 0), ('comunismo', 0), ('dynamost', 0), ('nechaev', 0), ('pataud', 0), ('pouget', 0), ('socities', 0), ('syndical', 0), ('workerist', 0), ('makhnovshchina', 0), ('dielo', 0), ('truda', 0), ('platformist', 0), ('fabbri', 0), ('reponse', 0), ('religiousity', 0), ('obediance', 0), ('stabalised', 0), ('ricourt', 0), ('mysogyny', 0), ('narveson', 0), ('raico', 0), ('ecofeminism', 0), ('formet', 0), ('crimethinc', 0), ('mcquinn', 0), ('postanarchism', 0), ('autonomism', 0), ('zapatismo', 0), ('wolfi', 0), ('landstreicher', 0), ('graeber', 0), ('grubacic', 0), ('komboa', 0), ('mbah', 0), ('infoshops', 0), ('cypherpunk', 0), ('parliamentarianism', 0), ('condoning', 0), ('voluntaryism', 0), ('panarchists', 0), ('criticsed', 0), ('reactionists', 0), ('lumpenproletariat', 0), ('spoilt', 0), ('dilettantes', 0), ('situationists', 0), ('proffessed', 0), ('totalitarians', 0), ('litist', 0), ('alfredsson', 0), ('rossell', 0), ('makhnovschina', 0), ('anarchoblogs', 0), ('neurodevelopmental', 0), ('stimulations', 0), ('oversensitivity', 0), ('underreactivity', 0), ('autreat', 0), ('echolalia', 0), ('neurotypicals', 0), ('stimming', 0), ('mealtimes', 0), ('perseveration', 0), ('underdiagnosed', 0), ('reclassifications', 0), ('cochair', 0), ('geekdom', 0), ('clinomorphism', 0), ('charika', 0), ('charin', 0), ('multigenerational', 0), ('pritikin', 0), ('jarrold', 0), ('tammet', 0), ('pdds', 0), ('regresses', 0), ('wrings', 0), ('chdd', 0), ('subthreshold', 0), ('aspies', 0), ('manev', 0), ('aminoglycoside', 0), ('epub', 0), ('publicat', 0), ('wrongplanet', 0), ('adelle', 0), ('autismwebsite', 0), ('rimland', 0), ('autismtoday', 0), ('aspie', 0)]\n",
            "Saved vocabulary to: ./vocabulary.tsv\n",
            "Writing summaries and checkpoints to logdir:w2v_logs_1558687248\n",
            "step 0 mean loss: 8.307425\n",
            "step 1000 mean loss: 13.191925\n",
            "step 2000 mean loss: 12.185254\n",
            "step 3000 mean loss: 11.415067\n",
            "step 4000 mean loss: 11.04783\n",
            "step 5000 mean loss: 10.853504\n",
            "step 6000 mean loss: 10.436065\n",
            "step 7000 mean loss: 10.240495\n",
            "step 8000 mean loss: 10.07959\n",
            "step 9000 mean loss: 9.920274\n",
            "step 10000 mean loss: 9.241254\n",
            "step 11000 mean loss: 9.4613285\n",
            "step 12000 mean loss: 9.142963\n",
            "step 13000 mean loss: 9.746187\n",
            "step 14000 mean loss: 8.990713\n",
            "step 15000 mean loss: 9.3135395\n",
            "step 16000 mean loss: 8.863734\n",
            "step 17000 mean loss: 8.966552\n",
            "step 18000 mean loss: 8.574883\n",
            "step 19000 mean loss: 8.688598\n",
            "step 20000 mean loss: 8.694814\n",
            "step 21000 mean loss: 8.270717\n",
            "step 22000 mean loss: 8.658395\n",
            "step 23000 mean loss: 8.254579\n",
            "step 24000 mean loss: 8.275298\n",
            "step 25000 mean loss: 8.1122675\n",
            "step 26000 mean loss: 8.241099\n",
            "step 27000 mean loss: 8.27182\n",
            "step 28000 mean loss: 7.8088875\n",
            "step 29000 mean loss: 7.9756417\n",
            "step 30000 mean loss: 7.808181\n",
            "step 31000 mean loss: 7.5792475\n",
            "step 32000 mean loss: 7.6771984\n",
            "step 33000 mean loss: 7.476352\n",
            "step 34000 mean loss: 7.6990185\n",
            "step 35000 mean loss: 7.3998938\n",
            "step 36000 mean loss: 7.5764093\n",
            "step 37000 mean loss: 7.5028334\n",
            "step 38000 mean loss: 7.1873837\n",
            "step 39000 mean loss: 7.3709064\n",
            "step 40000 mean loss: 7.4013224\n",
            "step 41000 mean loss: 7.1075673\n",
            "step 42000 mean loss: 7.348247\n",
            "step 43000 mean loss: 7.208085\n",
            "step 44000 mean loss: 7.4448977\n",
            "step 45000 mean loss: 7.3487225\n",
            "step 46000 mean loss: 7.2983446\n",
            "step 47000 mean loss: 6.956034\n",
            "step 48000 mean loss: 6.966922\n",
            "step 49000 mean loss: 7.048416\n",
            "step 50000 mean loss: 6.7778955\n",
            "step 51000 mean loss: 6.972722\n",
            "step 52000 mean loss: 6.820132\n",
            "step 53000 mean loss: 7.088869\n",
            "step 54000 mean loss: 6.4076376\n",
            "step 55000 mean loss: 6.5627275\n",
            "step 56000 mean loss: 6.611317\n",
            "step 57000 mean loss: 6.7110014\n",
            "step 58000 mean loss: 6.889368\n",
            "step 59000 mean loss: 6.5477424\n",
            "step 60000 mean loss: 6.593352\n",
            "step 61000 mean loss: 6.6071963\n",
            "step 62000 mean loss: 6.614621\n",
            "step 63000 mean loss: 6.3815126\n",
            "step 64000 mean loss: 6.488249\n",
            "step 65000 mean loss: 6.7125835\n",
            "step 66000 mean loss: 6.491531\n",
            "step 67000 mean loss: 6.462667\n",
            "step 68000 mean loss: 6.4255342\n",
            "step 69000 mean loss: 6.6915255\n",
            "step 70000 mean loss: 6.3878193\n",
            "step 71000 mean loss: 6.3456693\n",
            "step 72000 mean loss: 6.23348\n",
            "step 73000 mean loss: 6.004637\n",
            "step 74000 mean loss: 6.0938716\n",
            "step 75000 mean loss: 6.2593756\n",
            "step 76000 mean loss: 6.152167\n",
            "step 77000 mean loss: 6.7490683\n",
            "step 78000 mean loss: 6.21692\n",
            "step 79000 mean loss: 6.468343\n",
            "step 80000 mean loss: 6.1687403\n",
            "step 81000 mean loss: 6.2815266\n",
            "step 82000 mean loss: 6.189834\n",
            "step 83000 mean loss: 6.154537\n",
            "step 84000 mean loss: 5.98543\n",
            "step 85000 mean loss: 6.1429415\n",
            "step 86000 mean loss: 5.941166\n",
            "step 87000 mean loss: 5.9263225\n",
            "step 88000 mean loss: 5.842244\n",
            "step 89000 mean loss: 6.134053\n",
            "step 90000 mean loss: 5.9549193\n",
            "step 91000 mean loss: 5.8483524\n",
            "step 92000 mean loss: 5.7068753\n",
            "step 93000 mean loss: 5.8576026\n",
            "step 94000 mean loss: 5.548121\n",
            "step 95000 mean loss: 5.9283166\n",
            "step 96000 mean loss: 5.64681\n",
            "step 97000 mean loss: 6.011577\n",
            "step 98000 mean loss: 5.8861084\n",
            "step 99000 mean loss: 5.6545057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES9gEskoGqc-",
        "colab_type": "text"
      },
      "source": [
        "# Inspect embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcFA9e62jaKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading saved vocabulary and embedding matrix\n",
        "\n",
        "embeddings = np.load(os.path.join(OPTIONS.save_path, 'embeddings.npy'))\n",
        "with open(os.path.join(OPTIONS.save_path, 'vocabulary.tsv')) as f:\n",
        "  vocab = [l.strip() for l in f.readlines()]\n",
        " \n",
        "assert len(vocab) == embeddings.shape[0]\n",
        "\n",
        "embeddings_dict = {w : e for w, e in zip(vocab, embeddings)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5KeGAnKJ0m2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "8aa58ce8-fac9-45cd-9555-e04967f0fef1"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "\n",
        "def k_neighbors(vocab, embeddings, wv, word, k):\n",
        "  #TODO: implement function to find k similar words\n",
        "  word_embedding = wv[word]\n",
        "  similarities = [cosine(word_embedding, e) for e in embeddings]\n",
        "  top_neighbors = np.argsort(similarities)[:k]\n",
        "  return [(vocab[e], similarities[e]) for e in top_neighbors.reshape(-1)]\n",
        "\n",
        "\n",
        "k_neighbors(vocab, embeddings, embeddings_dict, 'three', 5)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('three', 0.0),\n",
              " ('four', 0.26832151412963867),\n",
              " ('five', 0.30084431171417236),\n",
              " ('two', 0.3192427158355713),\n",
              " ('seven', 0.34108853340148926)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}