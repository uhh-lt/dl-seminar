{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_task.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAICRJCxhe0v",
        "colab_type": "text"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2wQjBIxgNOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive') # in case you want to save reaults to your drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnghwfpchHOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8iKBfsTbsuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!wget http://mattmahoney.net/dc/text8.zip\n",
        "!unzip text8.zip\n",
        "\n",
        "!wget https://raw.githubusercontent.com/uhh-lt/dl-seminar/master/code/wikipedia-corpus-2mb.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_v7Rydw52qe",
        "colab_type": "text"
      },
      "source": [
        "# Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN2H2WSRWoj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "corpus_path = './text8'\n",
        "\n",
        "with open(corpus_path) as f:\n",
        "  corpus_raw = [w.lower() for w in nltk.word_tokenize(f.read())]\n",
        "\n",
        "n = 200\n",
        "corpus_token = []\n",
        "for i in range(0, len(corpus_raw), n):\n",
        "    corpus_token.append(corpus_raw[i:i + n])\n",
        "    \n",
        "\n",
        "# It is also possible to use the following code:\n",
        "\n",
        "# from gensim.models.word2vec import Text8Corpus\n",
        "# corpus_token = Text8Corpus(corpus_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh7_aUobXTV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "model = Word2Vec(corpus_token, size=100, window=2, min_count=3, workers=4, iter=3)\n",
        "model.save('gensim_word2vec.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzOUCl5EX4bH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Word2Vec.load('gensim_word2vec.model')\n",
        "\n",
        "# TODO: find similar words to word 'three'. You can use gensim 'most_similar' function\n",
        "# ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3B95Kv2hjxV",
        "colab_type": "text"
      },
      "source": [
        "# Utility code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iLYvHXlgkt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "\n",
        "\n",
        "def ensure_dir(f):\n",
        "    if not os.path.exists(f): \n",
        "      os.makedirs(f)\n",
        "      \n",
        "\n",
        "def load_corpus(filename, lower_case=True, min_frequency=3):\n",
        "    \"\"\" Load a text file, tokenize it, count occurences and build a word encoder \n",
        "    that translate a word into a unique id (sorted by word frequency) \"\"\"\n",
        "    \n",
        "    corpus = []\n",
        "    \n",
        "    i = 0\n",
        "    with open(filename, 'r') as in_file:\n",
        "        for line in in_file:\n",
        "            if i % 1000 == 0:\n",
        "                print('Loading {} processing line {}'.format(filename, i))\n",
        "            \n",
        "            if line[-1] == '\\n':\n",
        "                line = line[:-1]\n",
        "            line = line.strip()\n",
        "            if lower_case:\n",
        "                line = line.lower()\n",
        "            \n",
        "            corpus += nltk.word_tokenize(line)\n",
        "            i += 1\n",
        "    \n",
        "    print('Compute word encoder...')\n",
        "    word_counter = defaultdict(int)\n",
        "    \n",
        "    for word in corpus:\n",
        "        word_counter[word] += 1\n",
        "    \n",
        "    word_counter = list(word_counter.items())\n",
        "    word_counter = [elem for elem in word_counter if elem[1] >= min_frequency]\n",
        "    word_counter.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    word2index = defaultdict(int)\n",
        "    \n",
        "    for i, elem in enumerate(word_counter):\n",
        "        word2index[elem[0]] = i\n",
        "        \n",
        "    print('done')\n",
        "    \n",
        "    return corpus, word2index\n",
        "\n",
        "\n",
        "def save_vocabulary(output_dir, word2index):\n",
        "    vocab_fpath = os.path.join(output_dir, 'vocabulary.tsv')  \n",
        "    vocab_items = list(word2index.items())\n",
        "    vocab_items.sort(key=lambda x:x[1])\n",
        "    print(vocab_items[:100])\n",
        "    vocab_list = [elem[0] for elem in vocab_items if elem[1] > 0]\n",
        "    \n",
        "    with open(vocab_fpath, 'w') as vocab_file_out:\n",
        "        vocab_file_out.write('<UNK>'+'\\n')\n",
        "        for word in vocab_list:\n",
        "            vocab_file_out.write(word+'\\n')\n",
        "\n",
        "    print(\"Saved vocabulary to:\", vocab_fpath)\n",
        "    \n",
        "    return vocab_fpath"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yfw-Z_YhrRG",
        "colab_type": "text"
      },
      "source": [
        "# Model and training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-HGFrIxP7lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "from tensorflow.nn import sigmoid_cross_entropy_with_logits\n",
        "\n",
        "\n",
        "def build_graph2(vocabulary_size, num_sampled, embedding_size, \n",
        "                learning_rate, optimizer_type):\n",
        "    print('Using custom nce_loss function.')\n",
        "  \n",
        "    contexts = tf.placeholder(tf.int32, shape=[None])\n",
        "    targets = tf.placeholder(tf.int32, shape=[None, 1])\n",
        "    \n",
        "    embeddings = tf.Variable(\n",
        "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "    \n",
        "    nce_weights = tf.Variable(\n",
        "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
        "    \n",
        "    # TODO: implement noise contrastive estimation loss. Use tf.stop_gradients\n",
        "    # on sampled negative indices. We suggest using tf.nn.log_uniform_candidate_sampler to sample\n",
        "    # negative indices according to distribution of tokens in the corpus.\n",
        "    # Hint: you can always look into tf.nn.nce_loss code at github\n",
        "    \n",
        "    loss = # ...\n",
        "        \n",
        "    if optimizer_type == \"adam\":\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "    else:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "    \n",
        "    return embeddings, contexts, targets, optimizer, loss\n",
        "\n",
        "\n",
        "def build_graph(vocabulary_size, num_sampled, embedding_size, \n",
        "                learning_rate, optimizer_type):\n",
        "    print('Using built-in TF nce_loss function.')\n",
        "    \n",
        "    # Placeholders for inputs\n",
        "    contexts = tf.placeholder(tf.int32, shape=[None])\n",
        "    targets = tf.placeholder(tf.int32, shape=[None, 1])\n",
        "    \n",
        "    embeddings = tf.Variable(\n",
        "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "    \n",
        "    nce_weights = tf.Variable(\n",
        "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
        "    \n",
        "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "    \n",
        "    # TODO: generate embeddings of contexts \n",
        "    embed = # ...\n",
        "    \n",
        "    # TODO: compute the NCE loss, using a sample of the negative labels each time\n",
        "    # with tf.nn.nce_loss function (see TF documentation to find out what parameters you should use)\n",
        "    loss = # ...\n",
        "    \n",
        "    if optimizer_type == \"adam\":\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "    else:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "    \n",
        "    return embeddings, contexts, targets, optimizer, loss\n",
        "\n",
        "\n",
        "def generate_batch(corpus_num, batch_size, skip_gram=True):\n",
        "    \"\"\" Generate a batch in the form of two numpy vectors of (i) target \n",
        "    and (ii) context word ids. \"\"\"\n",
        "\n",
        "    contexts = np.ndarray(shape=(batch_size*2), dtype=np.int32)\n",
        "    targets = np.ndarray(shape=(batch_size*2, 1), dtype=np.int32)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        random_token_num = int(math.floor(np.random.random_sample() * (len(corpus_num) -2))) + 1\n",
        "        \n",
        "        # E.g. for \"the quick brown fox jumped over the lazy dog\"\n",
        "        # (context, target) pairs: ([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox)\n",
        "        # We can simplify to: (the, quick), (brown, quick), (quick, brown), (fox, brown), ... CBOW\n",
        "        # => contexts is ids of [the, brown, quick, fox, ...], labels/targets: [quick, quick, brown, brown, ...]\n",
        "\t# (quick, the), (quick, brown), (brown, quick), (brown, fox), ... Skip-gram\n",
        "        # => contexts and targets reversed\n",
        "        \n",
        "        # TODO: implement generation of left and right context pairs for CBOW \n",
        "        # according suggestions above\n",
        "        \n",
        "        # left context pair\n",
        "        left = # ...\n",
        "        \n",
        "        # right context pair\n",
        "        right = # ...\n",
        "        \n",
        "        if skip_gram:\n",
        "            # TODO: how we can transform left and right pairs to create SkipGram algorithm? \n",
        "            # ...\n",
        "        \n",
        "        contexts[i*2] = left[0]\n",
        "        contexts[i*2 + 1] = right[0]\n",
        "        \n",
        "        targets[i*2] = left[1]\n",
        "        targets[i*2 + 1] = right[1]\n",
        "    \n",
        "    return contexts, targets\n",
        "   \n",
        "  \n",
        "def train(corpus_num, word2index, vocabulary_size, num_samples, steps, \n",
        "          optimizer_type, learning_rate, embedding_size, skip_gram, \n",
        "          batch_size, save_path, use_custom_loss):   \n",
        "    with tf.device('/cpu'):\n",
        "        with tf.Session() as sess:\n",
        "            f_build_graph = build_graph2 if use_custom_loss else build_graph\n",
        "            \n",
        "            embeddings, contexts, targets, optimizer, loss = f_build_graph(vocabulary_size, \n",
        "                           num_samples, embedding_size, learning_rate, optimizer_type)\n",
        "            \n",
        "            # Save summary of the training process - can be analyzed with TensorBoard later \n",
        "            timestamp = str(int(time.time()))\n",
        "            logs_dir = os.path.join('w2v_logs_' + timestamp)\n",
        "            ensure_dir(logs_dir)\n",
        "            vocab_fpath = save_vocabulary(save_path, word2index)\n",
        "            \n",
        "            print('Writing summaries and checkpoints to logdir:' + logs_dir)\n",
        "            model_ckpt_fpath = os.path.join(logs_dir, 'model.ckpt')    \n",
        "            loss_summary = tf.summary.scalar('loss', loss) \n",
        "            config = projector.ProjectorConfig()\n",
        "            embedding = config.embeddings.add()\n",
        "            embedding.tensor_name = embeddings.name\n",
        "            embedding.metadata_path = vocab_fpath  \n",
        "            train_summary_op = tf.summary.merge_all()\n",
        "            summary_writer = tf.summary.FileWriter(logs_dir, sess.graph)\n",
        "            projector.visualize_embeddings(summary_writer, config)\n",
        "\n",
        "            # Initialization\n",
        "            saver = tf.train.Saver(tf.global_variables())\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            losses = []\n",
        "            \n",
        "            # Batched SGD training\n",
        "            for current_step in range(steps):\n",
        "                inputs, labels = generate_batch(corpus_num, batch_size=batch_size, skip_gram=skip_gram)\n",
        "                feed_dict = {contexts: inputs, targets: labels}\n",
        "                _, cur_loss = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
        "                \n",
        "                losses.append(cur_loss)\n",
        "                             \n",
        "                if current_step % 100==0 and current_step != 0:\n",
        "                    summary_str = sess.run(train_summary_op, feed_dict=feed_dict)\n",
        "                    summary_writer.add_summary(summary_str, current_step)\n",
        "                    \n",
        "                if current_step % 1000 == 0:\n",
        "                    print('step',current_step,'mean loss:', np.mean(np.asarray(losses)))\n",
        "                    saver.save(sess, model_ckpt_fpath, current_step)\n",
        "                    losses = []\n",
        "                    \n",
        "            embeddings_np = sess.run(embeddings)\n",
        "            np.save(os.path.join(save_path, 'embeddings.npy'), embeddings_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1yvMfb6iFmg",
        "colab_type": "text"
      },
      "source": [
        "# Launch training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_Rb8T8ag_QY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "OPTIONS = pd.Series()\n",
        "OPTIONS.corpus = \"wikipedia-corpus-2mb.txt\" # \"Path to the input text corpus. Change to 'text8' to train good embeddings.\"\n",
        "OPTIONS.num_neg_samples = 2    # \"Number of negative samples\"\n",
        "OPTIONS.steps = 100000         # \"Number of training steps\"\n",
        "OPTIONS.learning_rate = 1.     # \"The learning rate\"\n",
        "OPTIONS.embedding_size = 100   # \"Size of the embedding\"\n",
        "OPTIONS.lower_case = True      # \"Whether the corpus should be lowercased\"\n",
        "OPTIONS.skip_gram = False      # \"Whether skip gram should be used or CBOW\"\n",
        "OPTIONS.min_frequency = 3      # \"Words that occur lower than this frequency are discarded as OOV\"\n",
        "OPTIONS.optimizer_type = \"sgd\" # \"Optimizer type: 'adam' or 'sgd'\"\n",
        "OPTIONS.batch_size = 128       # \"Batch size\"\n",
        "OPTIONS.save_path = './'       # Path to directory to save results (dictionary, embedding matrice)\n",
        "OPTIONS.use_custom_loss = False # Switch to True if you want to do an advanced exercise\n",
        "                                # and implement nce loss by yourself in build_graph2 function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaLod8QSg5-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus, word2index = load_corpus(filename=OPTIONS.corpus, \n",
        "                                 lower_case=OPTIONS.lower_case, \n",
        "                                 min_frequency=OPTIONS.min_frequency)\n",
        "corpus_num = [word2index[word] for word in corpus]\n",
        "print(len(corpus_num))\n",
        "\n",
        "print('First few tokens of corpus:', corpus[:100])\n",
        "print('First few tokens of corpus_num:', list(corpus_num[:100]))\n",
        "\n",
        "corpus_num = np.asarray(corpus_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyjsdDLB6d5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "train(corpus_num, \n",
        "      word2index, \n",
        "      vocabulary_size=max(corpus_num) + 1,\n",
        "      num_samples=OPTIONS.num_neg_samples, \n",
        "      steps=OPTIONS.steps,\n",
        "      optimizer_type=OPTIONS.optimizer_type, \n",
        "      learning_rate=OPTIONS.learning_rate, \n",
        "      embedding_size=OPTIONS.embedding_size,\n",
        "      skip_gram=OPTIONS.skip_gram, \n",
        "      batch_size=OPTIONS.batch_size,\n",
        "      save_path=OPTIONS.save_path,\n",
        "      use_custom_loss=OPTIONS.use_custom_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES9gEskoGqc-",
        "colab_type": "text"
      },
      "source": [
        "# Inspect embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcFA9e62jaKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading saved vocabulary and embedding matrix\n",
        "\n",
        "embeddings = np.load(os.path.join(OPTIONS.save_path, 'embeddings.npy'))\n",
        "with open(os.path.join(OPTIONS.save_path, 'vocabulary.tsv')) as f:\n",
        "  vocab = [l.strip() for l in f.readlines()]\n",
        " \n",
        "assert len(vocab) == embeddings.shape[0]\n",
        "\n",
        "embeddings_dict = {w : e for w, e in zip(vocab, embeddings)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5KeGAnKJ0m2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "\n",
        "def k_neighbors(vocab, embeddings, wv, word, k):\n",
        "  # TODO: implement function to find k similar words\n",
        "  # ...\n",
        "\n",
        "\n",
        "k_neighbors(vocab, embeddings, embeddings_dict, 'three', 5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}